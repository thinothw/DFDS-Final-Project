{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPI5LvcEfNYFG9ethTZm7nv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c7f1b9d27eba498dbd1f40a23021564a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1a89b0cd9bcd4d66b3927c266185ec0f",
              "IPY_MODEL_55858bb35fb7427d8b6f0b0a0778a71b",
              "IPY_MODEL_2196d85cafa74285878241360e972cc9"
            ],
            "layout": "IPY_MODEL_34aa22fde8f1401d994d0d6bcad05278"
          }
        },
        "1a89b0cd9bcd4d66b3927c266185ec0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c4468bf337e7438eb580ad4de5be27cd",
            "placeholder": "​",
            "style": "IPY_MODEL_8f3b7c47950641e19a2ed744c464ca63",
            "value": "Downloading readme: "
          }
        },
        "55858bb35fb7427d8b6f0b0a0778a71b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e9d21fb9ad5d46018ddbd8dacd2c6bcb",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5aef3a24e2054c56b9703c362ca80b82",
            "value": 1
          }
        },
        "2196d85cafa74285878241360e972cc9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3462d4b482cb488f88ed1c33dd042fd0",
            "placeholder": "​",
            "style": "IPY_MODEL_c056938f2966495db102a238c584db62",
            "value": " 6.63k/? [00:00&lt;00:00, 402kB/s]"
          }
        },
        "34aa22fde8f1401d994d0d6bcad05278": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c4468bf337e7438eb580ad4de5be27cd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8f3b7c47950641e19a2ed744c464ca63": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e9d21fb9ad5d46018ddbd8dacd2c6bcb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "5aef3a24e2054c56b9703c362ca80b82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3462d4b482cb488f88ed1c33dd042fd0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c056938f2966495db102a238c584db62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5f0cd8ca6fce45cbadba586bb98ca66a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_aa104e960f4c46a1be8035f034b418fe",
              "IPY_MODEL_0f252a45e7db4c739a9957773fd3377d",
              "IPY_MODEL_bf6665af9b7e40128ce6c86e20068fa2"
            ],
            "layout": "IPY_MODEL_193d160514e843a58f072457856b92a1"
          }
        },
        "aa104e960f4c46a1be8035f034b418fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_53304587c0c3443d8bc2dbf02bdd7352",
            "placeholder": "​",
            "style": "IPY_MODEL_e478e06f2979449cb6e7339905b3a96f",
            "value": "Resolving data files: 100%"
          }
        },
        "0f252a45e7db4c739a9957773fd3377d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dc1387d348d04876875e366782ea9d87",
            "max": 206,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_88c08327768d41e9a3897f1495d200b2",
            "value": 206
          }
        },
        "bf6665af9b7e40128ce6c86e20068fa2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_18c75c41f33e4d609650835000f3f3d9",
            "placeholder": "​",
            "style": "IPY_MODEL_dc159ac6ffcb4ffdaff4198e6d252cb7",
            "value": " 206/206 [00:00&lt;00:00, 12190.34it/s]"
          }
        },
        "193d160514e843a58f072457856b92a1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "53304587c0c3443d8bc2dbf02bdd7352": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e478e06f2979449cb6e7339905b3a96f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dc1387d348d04876875e366782ea9d87": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "88c08327768d41e9a3897f1495d200b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "18c75c41f33e4d609650835000f3f3d9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dc159ac6ffcb4ffdaff4198e6d252cb7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c014e714941b422383a1ada48ec0d674": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c535a0dea3ad4e0c8ff81f3d720fd3f2",
              "IPY_MODEL_c49baca9b7bb4d2ab17c7020005684c4",
              "IPY_MODEL_3f7ea3b4c14f4f7baf2ef32f19038b22"
            ],
            "layout": "IPY_MODEL_b3cfa70a98ac43229374bef72eac2d38"
          }
        },
        "c535a0dea3ad4e0c8ff81f3d720fd3f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2cc498bea4d54df2af6336774c88906b",
            "placeholder": "​",
            "style": "IPY_MODEL_e7ba2fba9c06409cad34d4d5c41e1ca7",
            "value": "Resolving data files: 100%"
          }
        },
        "c49baca9b7bb4d2ab17c7020005684c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ae34da16e56d4d1687d84743f2a754bd",
            "max": 206,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_52ec0ff595574ea6a9dbfb77a4c2deff",
            "value": 206
          }
        },
        "3f7ea3b4c14f4f7baf2ef32f19038b22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_51932a4010264a4292ea6aafdb382b34",
            "placeholder": "​",
            "style": "IPY_MODEL_0f90ba5dbb984c968c4c6a723cb98b8c",
            "value": " 206/206 [00:00&lt;00:00, 8804.02it/s]"
          }
        },
        "b3cfa70a98ac43229374bef72eac2d38": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2cc498bea4d54df2af6336774c88906b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e7ba2fba9c06409cad34d4d5c41e1ca7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ae34da16e56d4d1687d84743f2a754bd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "52ec0ff595574ea6a9dbfb77a4c2deff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "51932a4010264a4292ea6aafdb382b34": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0f90ba5dbb984c968c4c6a723cb98b8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thinothw/DFDS-Final-Project/blob/main/Phase01_Sandbox.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Phase 01 - Setting up the Environment."
      ],
      "metadata": {
        "id": "1LYZUxSIBwmF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# INFRASTRUCTURE - Connect to Drive\n",
        "\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "print(\"Requesting Google Drive access...\")\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Double check the project folder exists\n",
        "base_path = '/content/drive/MyDrive/Deepfake_Honours_Project/Sandbox_10K'\n",
        "if os.path.exists(base_path):\n",
        "    print(f\" Connection Stable! Project folder found at: {base_path}\")\n",
        "else:\n",
        "    print(f\" Warning: Base path not found. Check your Drive folder name!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dhR7d0YSeEdd",
        "outputId": "5b913bde-f5ea-4f72-9416-2dc7b93d5b09"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requesting Google Drive access...\n",
            "Mounted at /content/drive\n",
            " Connection Stable! Project folder found at: /content/drive/MyDrive/Deepfake_Honours_Project/Sandbox_10K\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install retina-face opencv-python imagehash"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iGzw_k3yV81R",
        "outputId": "af165364-5819-4fcd-a8b8-74d14507cd6e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting retina-face\n",
            "  Downloading retina_face-0.0.17-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.13.0.92)\n",
            "Collecting imagehash\n",
            "  Downloading ImageHash-4.3.2-py2.py3-none-any.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from retina-face) (2.0.2)\n",
            "Requirement already satisfied: gdown>=3.10.1 in /usr/local/lib/python3.12/dist-packages (from retina-face) (5.2.1)\n",
            "Requirement already satisfied: Pillow>=5.2.0 in /usr/local/lib/python3.12/dist-packages (from retina-face) (11.3.0)\n",
            "Requirement already satisfied: tensorflow>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from retina-face) (2.19.0)\n",
            "Requirement already satisfied: PyWavelets in /usr/local/lib/python3.12/dist-packages (from imagehash) (1.9.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from imagehash) (1.16.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from gdown>=3.10.1->retina-face) (4.13.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from gdown>=3.10.1->retina-face) (3.24.3)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.12/dist-packages (from gdown>=3.10.1->retina-face) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from gdown>=3.10.1->retina-face) (4.67.3)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=1.9.0->retina-face) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=1.9.0->retina-face) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=1.9.0->retina-face) (25.12.19)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=1.9.0->retina-face) (0.7.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=1.9.0->retina-face) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=1.9.0->retina-face) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=1.9.0->retina-face) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow>=1.9.0->retina-face) (26.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=1.9.0->retina-face) (5.29.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow>=1.9.0->retina-face) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=1.9.0->retina-face) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=1.9.0->retina-face) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=1.9.0->retina-face) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=1.9.0->retina-face) (2.1.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=1.9.0->retina-face) (1.78.1)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=1.9.0->retina-face) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=1.9.0->retina-face) (3.10.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=1.9.0->retina-face) (3.15.1)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=1.9.0->retina-face) (0.5.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow>=1.9.0->retina-face) (0.46.3)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow>=1.9.0->retina-face) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow>=1.9.0->retina-face) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow>=1.9.0->retina-face) (0.19.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown>=3.10.1->retina-face) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown>=3.10.1->retina-face) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown>=3.10.1->retina-face) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown>=3.10.1->retina-face) (2026.1.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow>=1.9.0->retina-face) (3.10.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow>=1.9.0->retina-face) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow>=1.9.0->retina-face) (3.1.6)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->gdown>=3.10.1->retina-face) (2.8.3)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown>=3.10.1->retina-face) (1.7.1)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow>=1.9.0->retina-face) (3.0.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow>=1.9.0->retina-face) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow>=1.9.0->retina-face) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow>=1.9.0->retina-face) (0.1.2)\n",
            "Downloading retina_face-0.0.17-py3-none-any.whl (25 kB)\n",
            "Downloading ImageHash-4.3.2-py2.py3-none-any.whl (296 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.7/296.7 kB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: imagehash, retina-face\n",
            "Successfully installed imagehash-4.3.2 retina-face-0.0.17\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean, compatible install block\n",
        "!pip uninstall -y numpy -q\n",
        "!pip install -q numpy==1.26.4\n",
        "!pip install -q datasets==2.18.0\n",
        "!pip install -q facenet-pytorch==2.5.3\n",
        "!pip install -q opencv-python==4.8.0.76"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4qU98BcPAaCt",
        "outputId": "293aff95-d515-4ec6-a34b-f1f6a5d5aa69"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m50.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opencv-python 4.13.0.92 requires numpy>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "pytensor 2.38.0 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "tobler 0.13.0 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "jaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "jax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python-headless 4.13.0.92 requires numpy>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "shap 0.50.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-contrib-python 4.13.0.92 requires numpy>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "rasterio 1.5.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\n",
            "xarray-einstats 0.10.0 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m170.9/170.9 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.7/61.7 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "print(\"--- MASTER ENVIRONMENT HEALTH CHECK ---\")\n",
        "\n",
        "# 1. Verify Google Drive Mount\n",
        "drive_path = '/content/drive/MyDrive'\n",
        "if os.path.exists(drive_path):\n",
        "    print(f\" Vault Access: Google Drive is securely mounted at {drive_path}\")\n",
        "else:\n",
        "    print(f\" Vault Error: Google Drive NOT Mounted! Run drive.mount('/content/drive')\")\n",
        "\n",
        "# 2. Verify Core Libraries\n",
        "try:\n",
        "    import numpy as np\n",
        "    print(f\" NumPy Status: Active (Version {np.__version__} - Native Colab build)\")\n",
        "    import pandas as pd\n",
        "    print(f\" Pandas Status: Active (Version {pd.__version__})\")\n",
        "    import torch\n",
        "    print(f\" PyTorch Status: Active (Version {torch.__version__})\")\n",
        "    import cv2\n",
        "    print(f\" OpenCV Status: Active (Version {cv2.__version__})\")\n",
        "except ImportError as e:\n",
        "    print(f\" Core Library Error: {e}\")\n",
        "\n",
        "# 3. Verify Custom Computer Vision Tools\n",
        "try:\n",
        "    import imagehash\n",
        "    print(f\" ImageHash Status: Active (Ready for pHash deduplication)\")\n",
        "except ImportError:\n",
        "    print(\" ImageHash Missing! Run: !pip install imagehash\")\n",
        "\n",
        "try:\n",
        "    from retinaface import RetinaFace\n",
        "    print(f\" RetinaFace Status: Active (Ready for GPU extraction)\")\n",
        "except ImportError:\n",
        "    print(\" RetinaFace Missing! Run: !pip install retina-face\")\n",
        "\n",
        "print(\"\\n All systems green. The environment is absolutely stable and ready for the 10K Sandbox run.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iSigR_ApDpHx",
        "outputId": "166bc538-253a-43d1-cba8-e84b4032245b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- MASTER ENVIRONMENT HEALTH CHECK ---\n",
            " Vault Access: Google Drive is securely mounted at /content/drive/MyDrive\n",
            " NumPy Status: Active (Version 2.0.2 - Native Colab build)\n",
            " Pandas Status: Active (Version 2.2.2)\n",
            " PyTorch Status: Active (Version 2.10.0+cu128)\n",
            " OpenCV Status: Active (Version 4.13.0)\n",
            " ImageHash Status: Active (Ready for pHash deduplication)\n",
            " RetinaFace Status: Active (Ready for GPU extraction)\n",
            "\n",
            " All systems green. The environment is absolutely stable and ready for the 10K Sandbox run.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Phase 02 - Downloading Data from OpenFake."
      ],
      "metadata": {
        "id": "x3O2BBHTCGPM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 830,
          "referenced_widgets": [
            "c7f1b9d27eba498dbd1f40a23021564a",
            "1a89b0cd9bcd4d66b3927c266185ec0f",
            "55858bb35fb7427d8b6f0b0a0778a71b",
            "2196d85cafa74285878241360e972cc9",
            "34aa22fde8f1401d994d0d6bcad05278",
            "c4468bf337e7438eb580ad4de5be27cd",
            "8f3b7c47950641e19a2ed744c464ca63",
            "e9d21fb9ad5d46018ddbd8dacd2c6bcb",
            "5aef3a24e2054c56b9703c362ca80b82",
            "3462d4b482cb488f88ed1c33dd042fd0",
            "c056938f2966495db102a238c584db62",
            "5f0cd8ca6fce45cbadba586bb98ca66a",
            "aa104e960f4c46a1be8035f034b418fe",
            "0f252a45e7db4c739a9957773fd3377d",
            "bf6665af9b7e40128ce6c86e20068fa2",
            "193d160514e843a58f072457856b92a1",
            "53304587c0c3443d8bc2dbf02bdd7352",
            "e478e06f2979449cb6e7339905b3a96f",
            "dc1387d348d04876875e366782ea9d87",
            "88c08327768d41e9a3897f1495d200b2",
            "18c75c41f33e4d609650835000f3f3d9",
            "dc159ac6ffcb4ffdaff4198e6d252cb7",
            "c014e714941b422383a1ada48ec0d674",
            "c535a0dea3ad4e0c8ff81f3d720fd3f2",
            "c49baca9b7bb4d2ab17c7020005684c4",
            "3f7ea3b4c14f4f7baf2ef32f19038b22",
            "b3cfa70a98ac43229374bef72eac2d38",
            "2cc498bea4d54df2af6336774c88906b",
            "e7ba2fba9c06409cad34d4d5c41e1ca7",
            "ae34da16e56d4d1687d84743f2a754bd",
            "52ec0ff595574ea6a9dbfb77a4c2deff",
            "51932a4010264a4292ea6aafdb382b34",
            "0f90ba5dbb984c968c4c6a723cb98b8c"
          ]
        },
        "id": "1D0MjdjRokQz",
        "outputId": "8d17b57c-d5a1-4fdc-a513-be6418f47ca6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Checking GPU availability...\n",
            "Device detected: cpu\n",
            "\n",
            "Setting up high-speed local storage...\n",
            "\n",
            "Connecting to OpenFake dataset stream...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading readme: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c7f1b9d27eba498dbd1f40a23021564a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Resolving data files:   0%|          | 0/206 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5f0cd8ca6fce45cbadba586bb98ca66a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Resolving data files:   0%|          | 0/206 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c014e714941b422383a1ada48ec0d674"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset connection successful.\n",
            "\n",
            "Starting MASS DOWNLOAD to local storage (Target: 6000 per class)...\n",
            "  -> Downloaded 1000 / 6000 Fake images...\n",
            "  -> Downloaded 1000 / 6000 Real images...\n",
            "  -> Downloaded 2000 / 6000 Real images...\n",
            "  -> Downloaded 2000 / 6000 Fake images...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> Downloaded 3000 / 6000 Real images...\n",
            "  -> Downloaded 3000 / 6000 Fake images...\n",
            "  -> Downloaded 4000 / 6000 Fake images...\n",
            "  -> Downloaded 4000 / 6000 Real images...\n",
            "  -> Downloaded 5000 / 6000 Fake images...\n",
            "  -> Downloaded 5000 / 6000 Real images...\n",
            "  -> Downloaded 6000 / 6000 Real images...\n",
            "  -> Downloaded 6000 / 6000 Fake images...\n",
            "\n",
            "=== Download Summary ===\n",
            "Real images saved locally: 6000\n",
            "Fake images saved locally: 6000\n",
            "Save errors: 0\n",
            "\n",
            "Moving files from local storage to Google Drive. Please wait...\n",
            "Cleaning up temporary local files to free space...\n",
            " Mass download, Drive transfer, and cleanup completed successfully!\n"
          ]
        }
      ],
      "source": [
        "# Phase 2 - Download.\n",
        "# OpenFake Sandbox Dataset Downloader - 12,000 Images.\n",
        "# Code ran with 0 saved errors!.\n",
        "\n",
        "# 1. Core Imports\n",
        "import os\n",
        "import torch\n",
        "import shutil\n",
        "from datasets import load_dataset\n",
        "\n",
        "# 2. GPU Verification\n",
        "print(\"\\nChecking GPU availability...\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Device detected: {device}\")\n",
        "\n",
        "# 3. Create High-Speed Local Folders\n",
        "print(\"\\nSetting up high-speed local storage...\")\n",
        "local_base = '/content/temp_raw'\n",
        "local_real = os.path.join(local_base, 'real')\n",
        "local_fake = os.path.join(local_base, 'fake')\n",
        "\n",
        "os.makedirs(local_real, exist_ok=True)\n",
        "os.makedirs(local_fake, exist_ok=True)\n",
        "\n",
        "# 4. Connect to OpenFake Dataset\n",
        "print(\"\\nConnecting to OpenFake dataset stream...\")\n",
        "dataset_name = \"ComplexDataLab/OpenFake\"\n",
        "\n",
        "try:\n",
        "    openfake = load_dataset(dataset_name, split='train', streaming=True)\n",
        "    print(\"Dataset connection successful.\")\n",
        "except Exception as e:\n",
        "    raise RuntimeError(f\"Dataset loading failed: {e}\")\n",
        "\n",
        "# 5. Download Buffer (6,000 Real + 6,000 Fake)\n",
        "TARGET_BUFFER = 6000\n",
        "real_count = 0\n",
        "fake_count = 0\n",
        "save_errors = 0\n",
        "max_iterations = 80000\n",
        "iteration_counter = 0\n",
        "\n",
        "print(f\"\\nStarting MASS DOWNLOAD to local storage (Target: {TARGET_BUFFER} per class)...\")\n",
        "\n",
        "for item in openfake:\n",
        "    iteration_counter += 1\n",
        "\n",
        "    if iteration_counter > max_iterations:\n",
        "        print(\"\\n⚠️ Iteration cap exceeded. Stopping download.\")\n",
        "        break\n",
        "\n",
        "    try:\n",
        "        label = item['label']\n",
        "        image = item.get('image', None) # Safely attempt to get the image\n",
        "\n",
        "        # Safety 1: The Null Check\n",
        "        if image is None:\n",
        "            continue\n",
        "\n",
        "        # Safety 2: Handle Integer Labels\n",
        "        if isinstance(label, int):\n",
        "            label = 'real' if label == 0 else 'fake'\n",
        "\n",
        "        if label not in ['real', 'fake']:\n",
        "            continue\n",
        "\n",
        "        # Safety 3: Force RGB format\n",
        "        if image.mode != 'RGB':\n",
        "            image = image.convert('RGB')\n",
        "\n",
        "        if label == 'real' and real_count < TARGET_BUFFER:\n",
        "            image.save(os.path.join(local_real, f\"raw_openfake_real_{real_count}.jpg\"))\n",
        "            real_count += 1\n",
        "            if real_count % 1000 == 0:\n",
        "                print(f\"  -> Downloaded {real_count} / {TARGET_BUFFER} Real images...\")\n",
        "\n",
        "        elif label == 'fake' and fake_count < TARGET_BUFFER:\n",
        "            image.save(os.path.join(local_fake, f\"raw_openfake_fake_{fake_count}.jpg\"))\n",
        "            fake_count += 1\n",
        "            if fake_count % 1000 == 0:\n",
        "                print(f\"  -> Downloaded {fake_count} / {TARGET_BUFFER} Fake images...\")\n",
        "\n",
        "        if real_count == TARGET_BUFFER and fake_count == TARGET_BUFFER:\n",
        "            break\n",
        "\n",
        "    except Exception as e:\n",
        "        save_errors += 1\n",
        "        print(f\"Error at iteration {iteration_counter}: {e}\")\n",
        "\n",
        "print(\"\\n=== Download Summary ===\")\n",
        "print(f\"Real images saved locally: {real_count}\")\n",
        "print(f\"Fake images saved locally: {fake_count}\")\n",
        "print(f\"Save errors: {save_errors}\")\n",
        "\n",
        "# 6. Move to Google Drive\n",
        "print(\"\\nMoving files from local storage to Google Drive. Please wait...\")\n",
        "drive_base = '/content/drive/MyDrive/Deepfake_Honours_Project/Sandbox_10K/raw_data'\n",
        "drive_real = os.path.join(drive_base, 'real')\n",
        "drive_fake = os.path.join(drive_base, 'fake')\n",
        "\n",
        "os.makedirs(drive_real, exist_ok=True)\n",
        "os.makedirs(drive_fake, exist_ok=True)\n",
        "\n",
        "# Copy everything over\n",
        "shutil.copytree(local_real, drive_real, dirs_exist_ok=True)\n",
        "shutil.copytree(local_fake, drive_fake, dirs_exist_ok=True)\n",
        "\n",
        "# 7. Purge Local Temp Files\n",
        "print(\"Cleaning up temporary local files to free space...\")\n",
        "shutil.rmtree(local_base)\n",
        "\n",
        "print(\" Mass download, Drive transfer, and cleanup completed successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Phase 02.1 - Running RetinaFace Bouncer Script on OpenFake Data."
      ],
      "metadata": {
        "id": "sEyJq4oUCS6G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PHASE 2 - RetinaFace Bouncer V3.1.1\n",
        "# Load to Colab Local Drive then Offload to Drive.\n",
        "# Live Updates of the Progress.\n",
        "\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import shutil\n",
        "from retinaface import RetinaFace\n",
        "from PIL import Image\n",
        "import warnings\n",
        "from tqdm import tqdm  # The real time progress bar\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "print(\"Bouncer operating on: GPU (RetinaFace V4.3.2 High-Speed + Live Tracking)\\n\")\n",
        "\n",
        "# 1. Define Drive & Local Paths\n",
        "drive_base = '/content/drive/MyDrive/Deepfake_Honours_Project/Sandbox_10K'\n",
        "local_base = '/content/temp_workspace'\n",
        "\n",
        "drive_raw_real = os.path.join(drive_base, 'raw_data/real')\n",
        "drive_raw_fake = os.path.join(drive_base, 'raw_data/fake')\n",
        "\n",
        "local_raw_real = os.path.join(local_base, 'raw_data/real')\n",
        "local_raw_fake = os.path.join(local_base, 'raw_data/fake')\n",
        "\n",
        "local_hq_real = os.path.join(local_base, 'processed_data/real')\n",
        "local_hq_fake = os.path.join(local_base, 'processed_data/fake')\n",
        "local_reject_real = os.path.join(local_base, 'rejected/real')\n",
        "local_reject_fake = os.path.join(local_base, 'rejected/fake')\n",
        "\n",
        "# 2. Teleport Raw Data to Local SSD for Speed\n",
        "print(\" Step 1: Teleporting raw data from Google Drive to Local SSD (This takes 1-2 mins)...\")\n",
        "os.makedirs(local_base, exist_ok=True)\n",
        "shutil.copytree(drive_raw_real, local_raw_real, dirs_exist_ok=True)\n",
        "shutil.copytree(drive_raw_fake, local_raw_fake, dirs_exist_ok=True)\n",
        "print(\" Local transfer complete! Setting up output folders...\")\n",
        "\n",
        "for path in [local_hq_real, local_hq_fake, local_reject_real, local_reject_fake]:\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "\n",
        "# 3. Blur Detection Function\n",
        "def blur_score(pil_image):\n",
        "    img = np.array(pil_image)\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "    return cv2.Laplacian(gray, cv2.CV_64F).var()\n",
        "\n",
        "# 4. RetinaFace Extraction Engine\n",
        "def hq_crop_engine_v4_3(img_path, save_path, reject_path, padding=25, min_face_size=30, min_face_ratio=0.005, blur_threshold=50, confidence_threshold=0.90):\n",
        "    try:\n",
        "        faces = RetinaFace.detect_faces(img_path)\n",
        "        if not isinstance(faces, dict) or len(faces) == 0:\n",
        "            return False, \"No Face Detected\"\n",
        "\n",
        "        largest_area = 0\n",
        "        best_face = None\n",
        "        for face in faces.values():\n",
        "            box = face.get('facial_area', None)\n",
        "            if box is None: continue\n",
        "            area = (box[2] - box[0]) * (box[3] - box[1])\n",
        "            if area > largest_area:\n",
        "                largest_area = area\n",
        "                best_face = face\n",
        "\n",
        "        if best_face is None: return False, \"No Valid Face Found\"\n",
        "\n",
        "        box = best_face['facial_area']\n",
        "        confidence = best_face['score']\n",
        "        if confidence < confidence_threshold: return False, f\"Low Confidence ({confidence:.2f})\"\n",
        "\n",
        "        img_pil = Image.open(img_path).convert('RGB')\n",
        "        width, height = img_pil.size\n",
        "        face_w, face_h = box[2] - box[0], box[3] - box[1]\n",
        "\n",
        "        if face_w < min_face_size or face_h < min_face_size: return False, \"Too Small Pixels\"\n",
        "        if (face_w * face_h) / (width * height) < min_face_ratio: return False, \"Too Small Ratio\"\n",
        "\n",
        "        x_min, y_min = max(0, int(box[0]) - padding), max(0, int(box[1]) - padding)\n",
        "        x_max, y_max = min(width, int(box[2]) + padding), min(height, int(box[3]) + padding)\n",
        "\n",
        "        cropped_img = img_pil.crop((x_min, y_min, x_max, y_max))\n",
        "        blur_val = blur_score(cropped_img)\n",
        "\n",
        "        if blur_val < blur_threshold: return False, \"Blurred\"\n",
        "\n",
        "        cropped_img.save(save_path)\n",
        "        return True, \"Success\"\n",
        "    except Exception as e:\n",
        "        return False, str(e)\n",
        "\n",
        "# 5. High-Speed Cleaner with Live Progress Bar\n",
        "def clean_dataset_fast(input_folder, output_folder, reject_folder, label, target_valid=2000):\n",
        "    print(f\"\\n---  Processing {label} Folder (Target: {target_valid}) ---\")\n",
        "\n",
        "    images = [f for f in os.listdir(input_folder) if f.endswith(('.jpg', '.jpeg', '.png'))]\n",
        "    success, rejected = 0, 0\n",
        "    reasons = {}\n",
        "\n",
        "    # The tqdm wrapper creates the live progress bar\n",
        "    pbar = tqdm(total=target_valid, desc=f\"Extracting {label} Faces\", unit=\"face\")\n",
        "\n",
        "    for name in images:\n",
        "        if success >= target_valid:\n",
        "            break\n",
        "\n",
        "        img_path = os.path.join(input_folder, name)\n",
        "        save_path = os.path.join(output_folder, name)\n",
        "        reject_path = os.path.join(reject_folder, name)\n",
        "\n",
        "        ok, msg = hq_crop_engine_v4_3(img_path, save_path, reject_path)\n",
        "\n",
        "        if ok:\n",
        "            success += 1\n",
        "            pbar.update(1) # Ticks the progress bar forward\n",
        "        else:\n",
        "            rejected += 1\n",
        "            reasons[msg] = reasons.get(msg, 0) + 1\n",
        "            try:\n",
        "                Image.open(img_path).convert('RGB').save(reject_path)\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "    pbar.close()\n",
        "    print(f\"\\n STATS for {label}: {success} Valid | {rejected} Rejected\")\n",
        "\n",
        "# 6. Execute the Run\n",
        "clean_dataset_fast(local_raw_real, local_hq_real, local_reject_real, \"REAL\", target_valid=2000)\n",
        "clean_dataset_fast(local_raw_fake, local_hq_fake, local_reject_fake, \"FAKE\", target_valid=2000)\n",
        "\n",
        "# 7. Push Final Data Back to Drive\n",
        "print(\"\\n Step 3: Pushing pristine extracted faces back to Google Drive...\")\n",
        "drive_final_hq = os.path.join(drive_base, 'processed_data_V4_3')\n",
        "shutil.copytree(os.path.join(local_base, 'processed_data'), drive_final_hq, dirs_exist_ok=True)\n",
        "\n",
        "# 8. Clean Up\n",
        "print(\"🧹 Sweeping temporary local files...\")\n",
        "shutil.rmtree(local_base)\n",
        "\n",
        "print(\"\\n OpenFake Phase Complete! 4,000 perfectly balanced images locked into your Drive.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AS6GYH-xsZys",
        "outputId": "01642836-14bb-41dc-9517-d6fc720e7273"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bouncer operating on: GPU (RetinaFace V4.3.2 High-Speed + Live Tracking)\n",
            "\n",
            " Step 1: Teleporting raw data from Google Drive to Local SSD (This takes 1-2 mins)...\n",
            " Local transfer complete! Setting up output folders...\n",
            "\n",
            "---  Processing REAL Folder (Target: 2000) ---\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rExtracting REAL Faces:   0%|          | 0/2000 [00:00<?, ?face/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "26-02-27 15:49:41 - Directory /root/.deepface created\n",
            "26-02-27 15:49:41 - Directory /root/.deepface/weights created\n",
            "26-02-27 15:49:41 - retinaface.h5 will be downloaded from the url https://github.com/serengil/deepface_models/releases/download/v1.0/retinaface.h5\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://github.com/serengil/deepface_models/releases/download/v1.0/retinaface.h5\n",
            "To: /root/.deepface/weights/retinaface.h5\n",
            "\n",
            "  0%|          | 0.00/119M [00:00<?, ?B/s]\u001b[A\n",
            " 22%|██▏       | 25.7M/119M [00:00<00:00, 256MB/s]\u001b[A\n",
            " 50%|████▉     | 59.2M/119M [00:00<00:00, 302MB/s]\u001b[A\n",
            " 76%|███████▌  | 89.7M/119M [00:00<00:00, 270MB/s]\u001b[A\n",
            "100%|██████████| 119M/119M [00:00<00:00, 268MB/s]\n",
            "Extracting REAL Faces: 100%|██████████| 2000/2000 [20:02<00:00,  1.66face/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " STATS for REAL: 2000 Valid | 1240 Rejected\n",
            "\n",
            "---  Processing FAKE Folder (Target: 2000) ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting FAKE Faces: 100%|██████████| 2000/2000 [11:24<00:00,  2.92face/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " STATS for FAKE: 2000 Valid | 1420 Rejected\n",
            "\n",
            " Step 3: Pushing pristine extracted faces back to Google Drive...\n",
            "🧹 Sweeping temporary local files...\n",
            "\n",
            " OpenFake Phase Complete! 4,000 perfectly balanced images locked into your Drive.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Phase 03 - Downloading Data from FF++."
      ],
      "metadata": {
        "id": "M1SrlwCjClqZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import urllib.request\n",
        "import subprocess\n",
        "import shutil\n",
        "from google.colab import drive\n",
        "from google.colab import userdata\n",
        "\n",
        "# Unlock Vault\n",
        "print(\"\\nAccessing secure vault...\")\n",
        "TUM_URL = userdata.get('TUM_LINK')\n",
        "if not TUM_URL:\n",
        "    raise ValueError(\" ERROR: 'TUM_LINK' not found in Secrets. Please check the spelling!\")\n",
        "\n",
        "# Fetch Downloader directly from secret TUM link\n",
        "print(\"\\nFetching official TUM downloader script from your secure link...\")\n",
        "urllib.request.urlretrieve(TUM_URL, \"download.py\")\n",
        "\n",
        "# Define Production Paths\n",
        "temp_base = '/content/ff_temp'\n",
        "drive_base = '/content/drive/MyDrive/Deepfake_Honours_Project/Sandbox_10K/FF_Plus/raw_videos'\n",
        "\n",
        "# 10K Sandbox Split: 200 Real, 50 of each Fake (400 Total Videos)\n",
        "download_targets = {\n",
        "    'original': 200,\n",
        "    'Deepfakes': 50,\n",
        "    'Face2Face': 50,\n",
        "    'FaceSwap': 50,\n",
        "    'NeuralTextures': 50\n",
        "}\n",
        "\n",
        "print(\"\\n Firing up V3.2 10K Sandbox auto-bypassing download sequence...\")\n",
        "\n",
        "for cat, num_videos in download_targets.items():\n",
        "    sub_folder = 'real' if cat == 'original' else f'fake/{cat.lower()}'\n",
        "    temp_path = os.path.join(temp_base, sub_folder)\n",
        "    drive_path = os.path.join(drive_base, sub_folder)\n",
        "\n",
        "    shutil.rmtree(temp_path, ignore_errors=True)\n",
        "    os.makedirs(temp_path, exist_ok=True)\n",
        "    os.makedirs(drive_path, exist_ok=True)\n",
        "\n",
        "    print(f\"\\n Pulling {num_videos} videos for: {cat} to high-speed local storage...\")\n",
        "\n",
        "    # Echo automatically presses Enter to agree to the TOS, dynamic num_videos injected\n",
        "    cmd = f'echo \"\" | python3 download.py {temp_path} -d {cat} -c c23 -n {num_videos} --server EU2'\n",
        "    result = subprocess.run(cmd, shell=True)\n",
        "\n",
        "    if result.returncode != 0:\n",
        "        print(f\" ERROR: Download failed for {cat}. Check server status.\")\n",
        "    else:\n",
        "        print(f\" Download complete. Transferring {cat} to Google Drive...\")\n",
        "        shutil.copytree(temp_path, drive_path, dirs_exist_ok=True)\n",
        "        print(f\" Transfer complete for {cat}.\")\n",
        "\n",
        "# Global Housekeeping: Wipe the temporary master folder from the local SSD\n",
        "print(\"\\n Sweeping temporary local files...\")\n",
        "shutil.rmtree(temp_base, ignore_errors=True)\n",
        "\n",
        "print(\"\\n 10K Sandbox FF++ Download Complete! 400 videos securely stored.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sRhW9iLZCvsG",
        "outputId": "b8b5da3b-413b-4d69-83db-09e04dad025f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accessing secure vault...\n",
            "\n",
            "Fetching official TUM downloader script from your secure link...\n",
            "\n",
            " Firing up V3.2 10K Sandbox auto-bypassing download sequence...\n",
            "\n",
            " Pulling 200 videos for: original to high-speed local storage...\n",
            " Download complete. Transferring original to Google Drive...\n",
            " Transfer complete for original.\n",
            "\n",
            " Pulling 50 videos for: Deepfakes to high-speed local storage...\n",
            " Download complete. Transferring Deepfakes to Google Drive...\n",
            " Transfer complete for Deepfakes.\n",
            "\n",
            " Pulling 50 videos for: Face2Face to high-speed local storage...\n",
            " Download complete. Transferring Face2Face to Google Drive...\n",
            " Transfer complete for Face2Face.\n",
            "\n",
            " Pulling 50 videos for: FaceSwap to high-speed local storage...\n",
            " Download complete. Transferring FaceSwap to Google Drive...\n",
            " Transfer complete for FaceSwap.\n",
            "\n",
            " Pulling 50 videos for: NeuralTextures to high-speed local storage...\n",
            " Download complete. Transferring NeuralTextures to Google Drive...\n",
            " Transfer complete for NeuralTextures.\n",
            "\n",
            " Sweeping temporary local files...\n",
            "\n",
            " 10K Sandbox FF++ Download Complete! 400 videos securely stored.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Phase 03.1 - Running the Frame Extracter for the FF++ Vidoes."
      ],
      "metadata": {
        "id": "O8Ka-Wu1Sngf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import random\n",
        "import shutil\n",
        "import cv2\n",
        "import hashlib\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Lock the random seed for perfect reproducibility\n",
        "random.seed(42)\n",
        "\n",
        "# PRODUCTION PATHS: Pointing straight to the 10K Sandbox\n",
        "drive_raw_base = \"/content/drive/MyDrive/Deepfake_Honours_Project/Sandbox_10K/FF_Plus/raw_videos\"\n",
        "drive_extract_base = \"/content/drive/MyDrive/Deepfake_Honours_Project/Sandbox_10K/FF_Plus/extracted_frames\"\n",
        "drive_zip_out = \"/content/drive/MyDrive/Deepfake_Honours_Project/Sandbox_10K/FF_Plus/extracted_frames.zip\"\n",
        "\n",
        "# LOCAL SSD PATHS for max IO speed\n",
        "local_temp_base = \"/content/local_processing\"\n",
        "local_extract_base = os.path.join(local_temp_base, \"extracted_frames\")\n",
        "\n",
        "target_frames_with_buffer = 7\n",
        "split_ratios = {\"train\": 0.70, \"val\": 0.15, \"test\": 0.15}\n",
        "\n",
        "os.makedirs(local_temp_base, exist_ok=True)\n",
        "local_video_path = os.path.join(local_temp_base, \"processing_vid.mp4\")\n",
        "\n",
        "# Initialize Traceability Log for the main run\n",
        "log_file_path = \"/content/drive/MyDrive/Deepfake_Honours_Project/Sandbox_10K/FF_Plus/split_log.txt\"\n",
        "with open(log_file_path, \"w\") as f:\n",
        "    f.write(\"FF++ 10K Sandbox: Video Split and Extraction Log\\n\")\n",
        "    f.write(\"================================================\\n\")\n",
        "\n",
        "# Tracker for final distribution summary\n",
        "split_counts = {\"train\": 0, \"val\": 0, \"test\": 0}\n",
        "\n",
        "def extract_buffered_frames(local_path, original_path, output_folder, category_prefix, n_frames):\n",
        "    # Read the video from the fast Local SSD\n",
        "    cap = cv2.VideoCapture(local_path)\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "    # UPGRADE 2: Reject extremely short videos (Threshold: n_frames * 2)\n",
        "    if total_frames < (n_frames * 2):\n",
        "        cap.release()\n",
        "        return 0, total_frames\n",
        "\n",
        "    step = total_frames // n_frames\n",
        "    start_offset = random.randint(0, max(0, step - 1))\n",
        "    target_ids = set([start_offset + (i * step) for i in range(n_frames)])\n",
        "\n",
        "    # Generate the name and hash using the ORIGINAL Google Drive path\n",
        "    vid_name = os.path.basename(original_path).split('.')[0]\n",
        "    short_hash = hashlib.md5(original_path.encode()).hexdigest()[:6]\n",
        "\n",
        "    extracted_count = 0\n",
        "    current_id = 0\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret = cap.grab()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        if current_id in target_ids:\n",
        "            ret, frame = cap.retrieve()\n",
        "            if ret:\n",
        "                frame_name = f\"{category_prefix}_{vid_name}_{short_hash}_f{extracted_count:03d}.jpg\"\n",
        "                save_path = os.path.join(output_folder, frame_name)\n",
        "                cv2.imwrite(save_path, frame)\n",
        "                extracted_count += 1\n",
        "\n",
        "        current_id += 1\n",
        "        if extracted_count >= n_frames:\n",
        "            break\n",
        "\n",
        "    cap.release()\n",
        "    return extracted_count, total_frames\n",
        "\n",
        "print(\"Initiating reproducible extraction sequence onto local SSD...\")\n",
        "categories = ['real', 'fake/deepfakes', 'fake/face2face', 'fake/faceswap', 'fake/neuraltextures']\n",
        "\n",
        "total_extracted = 0\n",
        "skipped_videos = 0\n",
        "\n",
        "for cat in categories:\n",
        "    cat_path = os.path.join(drive_raw_base, cat)\n",
        "\n",
        "    # UPGRADE 1: Sort before shuffle for absolute determinism\n",
        "    videos = sorted(glob.glob(os.path.join(cat_path, \"**\", \"*.mp4\"), recursive=True))\n",
        "\n",
        "    if not videos:\n",
        "        continue\n",
        "\n",
        "    random.shuffle(videos)\n",
        "\n",
        "    train_split = int(len(videos) * split_ratios[\"train\"])\n",
        "    val_split = int(len(videos) * (split_ratios[\"train\"] + split_ratios[\"val\"]))\n",
        "\n",
        "    splits = {\n",
        "        \"train\": videos[:train_split],\n",
        "        \"val\": videos[train_split:val_split],\n",
        "        \"test\": videos[val_split:]\n",
        "    }\n",
        "\n",
        "    for split_name, split_videos in splits.items():\n",
        "        if not split_videos:\n",
        "            continue\n",
        "\n",
        "        cat_safe_name = cat.replace('/', '_')\n",
        "        final_local_folder = os.path.join(local_extract_base, split_name, cat_safe_name)\n",
        "        os.makedirs(final_local_folder, exist_ok=True)\n",
        "\n",
        "        pbar = tqdm(total=len(split_videos), desc=f\"{split_name.upper()} - {cat_safe_name}\", unit=\"vid\")\n",
        "\n",
        "        with open(log_file_path, \"a\") as log:\n",
        "            for video_path in split_videos:\n",
        "                shutil.copy2(video_path, local_video_path)\n",
        "\n",
        "                frames_saved, total_found = extract_buffered_frames(local_video_path, video_path, final_local_folder, f\"{split_name}_{cat_safe_name}\", target_frames_with_buffer)\n",
        "\n",
        "                if frames_saved == 0:\n",
        "                    skipped_videos += 1\n",
        "                    log.write(f\"[SKIPPED] {video_path} (Only {total_found} frames - failed diversity threshold)\\n\")\n",
        "                else:\n",
        "                    total_extracted += frames_saved\n",
        "                    split_counts[split_name] += frames_saved  # UPGRADE 3: Tracking distribution\n",
        "                    log.write(f\"[{split_name.upper()}] {video_path} -> Extracted {frames_saved} frames\\n\")\n",
        "\n",
        "                pbar.update(1)\n",
        "\n",
        "        pbar.close()\n",
        "\n",
        "print(\"\\nArchiving frames into a single high-speed zip payload...\")\n",
        "# This creates extracted_frames.zip on the local SSD\n",
        "shutil.make_archive(os.path.join(local_temp_base, \"extracted_frames\"), 'zip', local_extract_base)\n",
        "\n",
        "print(\"Pushing zip payload to Google Drive vault...\")\n",
        "# We only transfer ONE file over the network\n",
        "shutil.copy2(os.path.join(local_temp_base, \"extracted_frames.zip\"), drive_zip_out)\n",
        "\n",
        "print(\"Sweeping local temporary files...\")\n",
        "shutil.rmtree(local_temp_base, ignore_errors=True)\n",
        "\n",
        "# UPGRADE 3: Final Distribution Summary\n",
        "print(\"\\n--- FINAL EXTRACTION SUMMARY ---\")\n",
        "print(f\"Total Frames Secured: {total_extracted}\")\n",
        "print(f\"   ► TRAIN: {split_counts['train']} frames\")\n",
        "print(f\"   ► VAL:   {split_counts['val']} frames\")\n",
        "print(f\"   ► TEST:  {split_counts['test']} frames\")\n",
        "print(f\"\\nSkipped Videos (Too Short): {skipped_videos}\")\n",
        "print(\"Pipeline complete. Check split_log.txt for full traceability.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BmsUfhlxZnHb",
        "outputId": "ab108e32-112c-4759-ab9c-c7970fdd12b0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initiating reproducible extraction sequence onto local SSD...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "TRAIN - real: 100%|██████████| 140/140 [04:26<00:00,  1.91s/vid]\n",
            "VAL - real: 100%|██████████| 30/30 [01:05<00:00,  2.18s/vid]\n",
            "TEST - real: 100%|██████████| 30/30 [00:58<00:00,  1.96s/vid]\n",
            "TRAIN - fake_deepfakes: 100%|██████████| 35/35 [01:20<00:00,  2.29s/vid]\n",
            "VAL - fake_deepfakes: 100%|██████████| 7/7 [00:21<00:00,  3.12s/vid]\n",
            "TEST - fake_deepfakes: 100%|██████████| 8/8 [00:18<00:00,  2.37s/vid]\n",
            "TRAIN - fake_face2face: 100%|██████████| 35/35 [01:30<00:00,  2.60s/vid]\n",
            "VAL - fake_face2face: 100%|██████████| 7/7 [00:15<00:00,  2.21s/vid]\n",
            "TEST - fake_face2face: 100%|██████████| 8/8 [00:14<00:00,  1.76s/vid]\n",
            "TRAIN - fake_faceswap: 100%|██████████| 35/35 [01:10<00:00,  2.02s/vid]\n",
            "VAL - fake_faceswap: 100%|██████████| 7/7 [00:15<00:00,  2.16s/vid]\n",
            "TEST - fake_faceswap: 100%|██████████| 8/8 [00:16<00:00,  2.09s/vid]\n",
            "TRAIN - fake_neuraltextures: 100%|██████████| 35/35 [01:16<00:00,  2.19s/vid]\n",
            "VAL - fake_neuraltextures: 100%|██████████| 7/7 [00:10<00:00,  1.56s/vid]\n",
            "TEST - fake_neuraltextures: 100%|██████████| 8/8 [00:12<00:00,  1.62s/vid]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Archiving frames into a single high-speed zip payload...\n",
            "Pushing zip payload to Google Drive vault...\n",
            "Sweeping local temporary files...\n",
            "\n",
            "--- FINAL EXTRACTION SUMMARY ---\n",
            "Total Frames Secured: 2800\n",
            "   ► TRAIN: 1960 frames\n",
            "   ► VAL:   406 frames\n",
            "   ► TEST:  434 frames\n",
            "\n",
            "Skipped Videos (Too Short): 0\n",
            "Pipeline complete. Check split_log.txt for full traceability.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Phase 03.2 - Running the RetinaFace on Extracted Frames."
      ],
      "metadata": {
        "id": "02Zl-KMpbpQY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- BK Tree Library Installer."
      ],
      "metadata": {
        "id": "egpxgEnPBrLh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pybktree imagehash"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pgc-SryHBzhH",
        "outputId": "18729188-b302-42a0-c3aa-1a6b5c90763b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pybktree\n",
            "  Downloading pybktree-1.1.tar.gz (4.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: imagehash in /usr/local/lib/python3.12/dist-packages (4.3.2)\n",
            "Requirement already satisfied: PyWavelets in /usr/local/lib/python3.12/dist-packages (from imagehash) (1.9.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from imagehash) (2.0.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from imagehash) (11.3.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from imagehash) (1.16.3)\n",
            "Building wheels for collected packages: pybktree\n",
            "  Building wheel for pybktree (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pybktree: filename=pybktree-1.1-py3-none-any.whl size=4949 sha256=626532c672bda8a396aeb175bdd803b124ed109058cb6d123eb58cfa5f0128ae\n",
            "  Stored in directory: /root/.cache/pip/wheels/e0/c0/e9/f03776b415a424272cb3cb1baf27385d100f3ef7eb9bb6553e\n",
            "Successfully built pybktree\n",
            "Installing collected packages: pybktree\n",
            "Successfully installed pybktree-1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import shutil\n",
        "import glob\n",
        "import csv\n",
        "import imagehash\n",
        "import random\n",
        "import pybktree\n",
        "from retinaface import RetinaFace\n",
        "from PIL import Image\n",
        "import warnings\n",
        "from tqdm import tqdm\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# 1. Global Determinism\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"Master Bouncer Operating On: GPU (Zip Pipeline + BK-Tree + Quota Enforcer)\\n\")\n",
        "\n",
        "# --- PRODUCTION ZIP PATHS ---\n",
        "drive_base = '/content/drive/MyDrive/Deepfake_Honours_Project/Sandbox_10K/FF_Plus'\n",
        "drive_zip_in = os.path.join(drive_base, 'extracted_frames.zip')\n",
        "drive_zip_out = os.path.join(drive_base, 'processed_faces.zip')\n",
        "csv_log_path = os.path.join(drive_base, 'extraction_log.csv')\n",
        "\n",
        "local_base = '/content/temp_workspace'\n",
        "local_zip_in = os.path.join(local_base, 'extracted_frames.zip')\n",
        "local_input = os.path.join(local_base, 'input_frames')\n",
        "local_output = os.path.join(local_base, 'processed_faces')\n",
        "\n",
        "# --- THE STRICT QUOTA SYSTEM ---\n",
        "target_quotas = {\n",
        "    \"train_real\": 700, \"val_real\": 150, \"test_real\": 150,\n",
        "    \"train_fake_deepfakes\": 175, \"val_fake_deepfakes\": 37, \"test_fake_deepfakes\": 38,\n",
        "    \"train_fake_face2face\": 175, \"val_fake_face2face\": 37, \"test_fake_face2face\": 38,\n",
        "    \"train_fake_faceswap\": 175, \"val_fake_faceswap\": 37, \"test_fake_faceswap\": 38,\n",
        "    \"train_fake_neuraltextures\": 175, \"val_fake_neuraltextures\": 37, \"test_fake_neuraltextures\": 38\n",
        "}\n",
        "\n",
        "accepted_counts = {key: 0 for key in target_quotas}\n",
        "total_rejected = 0\n",
        "\n",
        "# --- THE ZIP TELEPORTATION PROTOCOL ---\n",
        "print(\"Teleporting single zip payload to Local SSD...\")\n",
        "shutil.rmtree(local_base, ignore_errors=True)\n",
        "os.makedirs(local_base, exist_ok=True)\n",
        "\n",
        "shutil.copy2(drive_zip_in, local_zip_in)\n",
        "\n",
        "print(\"Unpacking payload directly into fast SSD memory...\")\n",
        "shutil.unpack_archive(local_zip_in, local_input)\n",
        "os.remove(local_zip_in) # Vaporize the zip to free up SSD space\n",
        "\n",
        "with open(csv_log_path, mode='w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"Image_Name\", \"Split\", \"Category\", \"Status\", \"Reason\", \"Confidence\", \"Blur_Score\", \"pHash\", \"Bounding_Box\"])\n",
        "\n",
        "# --- THE BK-TREE UPGRADE ---\n",
        "def hash_distance(hash1, hash2):\n",
        "    return hash1 - hash2\n",
        "\n",
        "seen_hashes_tree = pybktree.BKTree(hash_distance)\n",
        "\n",
        "def master_crop_engine(img_path, save_path, padding=25, min_face_size=30, min_face_ratio=0.005, blur_threshold=25, confidence_threshold=0.90):\n",
        "    try:\n",
        "        img_cv = cv2.imread(img_path)\n",
        "        if img_cv is None:\n",
        "            return False, \"Corrupted Image\", 0.0, 0.0, \"\", \"[]\"\n",
        "\n",
        "        height, width = img_cv.shape[:2]\n",
        "\n",
        "        faces = RetinaFace.detect_faces(img_cv)\n",
        "        if not isinstance(faces, dict) or len(faces) == 0:\n",
        "            return False, \"No Face Detected\", 0.0, 0.0, \"\", \"[]\"\n",
        "\n",
        "        largest_area = 0\n",
        "        best_face = None\n",
        "        for face in faces.values():\n",
        "            box = face.get('facial_area', None)\n",
        "            if box is None: continue\n",
        "            area = (box[2] - box[0]) * (box[3] - box[1])\n",
        "            if area > largest_area:\n",
        "                largest_area = area\n",
        "                best_face = face\n",
        "\n",
        "        if best_face is None:\n",
        "            return False, \"No Valid Face Found\", 0.0, 0.0, \"\", \"[]\"\n",
        "\n",
        "        box = best_face['facial_area']\n",
        "        str_box = f\"[{box[0]}, {box[1]}, {box[2]}, {box[3]}]\"\n",
        "        confidence = best_face['score']\n",
        "\n",
        "        if confidence < confidence_threshold:\n",
        "            return False, \"Low Confidence\", confidence, 0.0, \"\", str_box\n",
        "\n",
        "        face_w, face_h = box[2] - box[0], box[3] - box[1]\n",
        "\n",
        "        if face_w < min_face_size or face_h < min_face_size:\n",
        "            return False, \"Resolution Too Small\", confidence, 0.0, \"\", str_box\n",
        "        if (face_w * face_h) / (width * height) < min_face_ratio:\n",
        "            return False, \"Face Ratio Too Small\", confidence, 0.0, \"\", str_box\n",
        "\n",
        "        x_min, y_min = max(0, int(box[0]) - padding), max(0, int(box[1]) - padding)\n",
        "        x_max, y_max = min(width, int(box[2]) + padding), min(height, int(box[3]) + padding)\n",
        "        cropped_cv = img_cv[y_min:y_max, x_min:x_max]\n",
        "\n",
        "        gray_crop = cv2.cvtColor(cropped_cv, cv2.COLOR_BGR2GRAY)\n",
        "        blur_val = cv2.Laplacian(gray_crop, cv2.CV_64F).var()\n",
        "\n",
        "        if blur_val < blur_threshold:\n",
        "            return False, \"Motion Blur Detected\", confidence, blur_val, \"\", str_box\n",
        "\n",
        "        cropped_pil = Image.fromarray(cv2.cvtColor(cropped_cv, cv2.COLOR_BGR2RGB))\n",
        "        standardized_img = cropped_pil.resize((224, 224), Image.BICUBIC)\n",
        "\n",
        "        new_hash = imagehash.phash(standardized_img)\n",
        "\n",
        "        matches = seen_hashes_tree.find(new_hash, 2)\n",
        "        if matches:\n",
        "            return False, \"Duplicate Face (Hamming Dist <= 2)\", confidence, blur_val, str(new_hash), str_box\n",
        "\n",
        "        seen_hashes_tree.add(new_hash)\n",
        "        standardized_img.save(save_path, format='JPEG', quality=95)\n",
        "\n",
        "        return True, \"Accepted\", confidence, blur_val, str(new_hash), str_box\n",
        "\n",
        "    except Exception as e:\n",
        "        return False, f\"Engine Error: {str(e)}\", 0.0, 0.0, \"\", \"[]\"\n",
        "\n",
        "print(\"Firing up RetinaFace Deduplication Engine...\")\n",
        "image_files = glob.glob(os.path.join(local_input, \"**\", \"*.jpg\"), recursive=True)\n",
        "\n",
        "if not image_files:\n",
        "    print(\"ERROR: No images found in local storage.\")\n",
        "else:\n",
        "    random.shuffle(image_files)\n",
        "\n",
        "    pbar = tqdm(total=sum(target_quotas.values()), desc=\"Securing Quota\", unit=\"face\")\n",
        "\n",
        "    with open(csv_log_path, mode='a', newline='') as log_file:\n",
        "        csv_writer = csv.writer(log_file)\n",
        "\n",
        "        for img_path in image_files:\n",
        "            # --- THE STRING SAFETY UPGRADE (Bottom-Up reading for Zip compatibility) ---\n",
        "            parent_dir = os.path.dirname(img_path)\n",
        "            cat_name_raw = os.path.basename(parent_dir)\n",
        "            split_name_raw = os.path.basename(os.path.dirname(parent_dir))\n",
        "\n",
        "            split_name = split_name_raw.lower().strip()\n",
        "            cat_name = cat_name_raw.lower().strip()\n",
        "\n",
        "            bucket_key = f\"{split_name}_{cat_name}\"\n",
        "\n",
        "            if bucket_key in target_quotas and accepted_counts.get(bucket_key, 0) >= target_quotas[bucket_key]:\n",
        "                continue\n",
        "\n",
        "            # Ensure we maintain structure in the output folder\n",
        "            out_folder = os.path.join(local_output, split_name, cat_name)\n",
        "            os.makedirs(out_folder, exist_ok=True)\n",
        "\n",
        "            file_name = os.path.basename(img_path)\n",
        "            save_path = os.path.join(out_folder, file_name)\n",
        "\n",
        "            passed, msg, conf, blur, phash_val, bbox = master_crop_engine(img_path, save_path)\n",
        "\n",
        "            if passed:\n",
        "                if bucket_key in accepted_counts:\n",
        "                    accepted_counts[bucket_key] += 1\n",
        "                csv_writer.writerow([file_name, split_name, cat_name, \"Accepted\", \"None\", round(conf, 4), round(blur, 2), phash_val, bbox])\n",
        "                pbar.update(1)\n",
        "            else:\n",
        "                total_rejected += 1\n",
        "                csv_writer.writerow([file_name, split_name, cat_name, \"Rejected\", msg, round(conf, 4), round(blur, 2), phash_val, bbox])\n",
        "\n",
        "            if all(accepted_counts[k] >= target_quotas[k] for k in target_quotas):\n",
        "                print(\"\\nAll target quotas achieved early! Shutting down Bouncer.\")\n",
        "                break\n",
        "\n",
        "    pbar.close()\n",
        "\n",
        "    print(\"\\n--- FINAL BOUNCER SUMMARY ---\")\n",
        "    total_secured = sum(accepted_counts.values())\n",
        "    print(f\"Total Pristine Faces Secured: {total_secured} / {sum(target_quotas.values())}\")\n",
        "    print(f\"Total Frames Rejected: {total_rejected}\")\n",
        "\n",
        "    print(\"\\n[REAL CATEGORY STATS]\")\n",
        "    print(f\"  ► Train: {accepted_counts.get('train_real', 0)}/{target_quotas['train_real']}\")\n",
        "    print(f\"  ► Val:   {accepted_counts.get('val_real', 0)}/{target_quotas['val_real']}\")\n",
        "    print(f\"  ► Test:  {accepted_counts.get('test_real', 0)}/{target_quotas['test_real']}\")\n",
        "\n",
        "    print(\"\\n[FAKE SUBCATEGORY STATS (Target per split: 175 Train / 37 Val / 38 Test)]\")\n",
        "    fake_cats = [\"fake_deepfakes\", \"fake_face2face\", \"fake_faceswap\", \"fake_neuraltextures\"]\n",
        "    for f_cat in fake_cats:\n",
        "        train_hit = accepted_counts.get(f'train_{f_cat}', 0)\n",
        "        val_hit = accepted_counts.get(f'val_{f_cat}', 0)\n",
        "        test_hit = accepted_counts.get(f'test_{f_cat}', 0)\n",
        "        print(f\"  ► {f_cat}: Train({train_hit}) | Val({val_hit}) | Test({test_hit})\")\n",
        "\n",
        "print(\"\\nArchiving pristine faces into a single zip payload...\")\n",
        "shutil.make_archive(os.path.join(local_base, \"processed_faces\"), 'zip', local_output)\n",
        "\n",
        "print(\"Pushing standardized zip dataset and CSV logs back to Google Drive...\")\n",
        "shutil.copy2(os.path.join(local_base, \"processed_faces.zip\"), drive_zip_out)\n",
        "shutil.rmtree(local_base, ignore_errors=True)\n",
        "\n",
        "print(\"Pipeline Complete! Check extraction_log.csv for full traceability.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 668
        },
        "id": "uWQz6DtXHqgj",
        "outputId": "34c0229f-51a0-42e9-9b0f-e3490a3b1d3c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Master Bouncer Operating On: GPU (BK-Tree pHash + Quota Enforcer + SSD Only)\n",
            "\n",
            "Teleporting buffered frames to Local SSD (Bypassing FUSE bottleneck)...\n",
            "Firing up RetinaFace Deduplication Engine...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rSecuring Quota:   0%|          | 0/2000 [00:00<?, ?face/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "26-03-01 21:51:07 - Directory /root/.deepface created\n",
            "26-03-01 21:51:07 - Directory /root/.deepface/weights created\n",
            "26-03-01 21:51:07 - retinaface.h5 will be downloaded from the url https://github.com/serengil/deepface_models/releases/download/v1.0/retinaface.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://github.com/serengil/deepface_models/releases/download/v1.0/retinaface.h5\n",
            "To: /root/.deepface/weights/retinaface.h5\n",
            "\n",
            "  0%|          | 0.00/119M [00:00<?, ?B/s]\u001b[A\n",
            " 18%|█▊        | 21.5M/119M [00:00<00:00, 209MB/s]\u001b[A\n",
            " 36%|███▌      | 42.5M/119M [00:00<00:00, 192MB/s]\u001b[A\n",
            " 71%|███████   | 84.4M/119M [00:00<00:00, 253MB/s]\u001b[A\n",
            "100%|██████████| 119M/119M [00:00<00:00, 165MB/s]\n",
            "Securing Quota:   5%|▍         | 94/2000 [00:38<05:09,  6.16face/s]"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3244/3818591597.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    163\u001b[0m             \u001b[0msave_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m             \u001b[0mpassed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblur\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphash_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbbox\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaster_crop_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpassed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3244/3818591597.py\u001b[0m in \u001b[0;36mmaster_crop_engine\u001b[0;34m(img_path, save_path, padding, min_face_size, min_face_ratio, blur_threshold, confidence_threshold)\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_cv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m         \u001b[0mfaces\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRetinaFace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetect_faces\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_cv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfaces\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfaces\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"No Face Detected\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"[]\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/retinaface/RetinaFace.py\u001b[0m in \u001b[0;36mdetect_faces\u001b[0;34m(img_path, threshold, model, allow_upscaling)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0mlandmarks_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0mim_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mim_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mim_scale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_upscaling\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m     \u001b[0mnet_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m     \u001b[0mnet_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0melt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0melt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnet_out\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0msym_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    876\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m       results = tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    879\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1321\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1686\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1687\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1688\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1689\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1690\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Stats from RetinaFace Extraction"
      ],
      "metadata": {
        "id": "JmkVFzhQQgPT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "csv_path = '/content/drive/MyDrive/Deepfake_Honours_Project/Sandbox_10K/FF_Plus/Test_Run/extraction_log_v2.csv'\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "print(\"--- BOUNCER REJECTION BREAKDOWN ---\")\n",
        "rejections = df[df['Status'] == 'Rejected']\n",
        "print(rejections['Reason'].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-A8PGpzjMzTC",
        "outputId": "7bf68090-ff27-4ad3-b42b-453af8ec71d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- BOUNCER REJECTION BREAKDOWN ---\n",
            "Reason\n",
            "Duplicate Face (Hamming Dist <= 2)    14\n",
            "Motion Blur Detected                   3\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    }
  ]
}